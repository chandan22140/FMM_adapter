{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828},{"sourceId":11650405,"sourceType":"datasetVersion","datasetId":7311204}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom typing import Dict\nimport math\nfrom typing import Optional, List\n\nimport time\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport wandb\nwandb.login(key=\"\")\nfrom sklearn.metrics import f1_score\nimport yaml\n\n# Load configuration\ndef load_config(path='/kaggle/input/cccccc/config.yaml'):\n    with open(path, 'r') as f:\n        return yaml.safe_load(f)\n\nconfig = load_config()\n\n# Unpack config\nR_LORA = config.get('adapter_rank')\nSAMPLING_STEPS = config.get('num_subsamples')\nSUBADAPTER_UPDATE_FREQ = config.get('subadapter_update_freq')\nREG_LAMBDA = config.get('reg_lambda')\nREG_DELTA = config.get('reg_delta')\n\n# Initialize WandB with config\nwandb.init(project=\"subadapter_peft\", config=config)\n\n#####################################\n# Hyperparameters\n#####################################\nEPOCHS = 1\nbatch_size = 64            # Adjust as your GPU allows\nBASE_LR = 1e-3\nWEIGHT_DECAY = 0.03\nDROPOUT = 0.1\n\n\nclass LoRALayer():\n    def __init__(\n        self,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        self.r = r\n        self.lora_alpha = lora_alpha\n\n        # Optional dropout\n        if lora_dropout > 0.:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights\n\n\nclass xLinear(nn.Linear, LoRALayer):\n    # LoRA implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 32,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,\n        merge_weights: bool = True,\n        pretrained_weights=None,  # Added to accept pretrained weights\n        pretrained_bias=None,     # Added to accept pretrained bias\n        **kwargs\n    ):\n        super().__init__(in_features, out_features, **kwargs)\n        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n                           merge_weights=merge_weights)\n\n        self.fan_in_fan_out = fan_in_fan_out\n        if pretrained_weights is not None:\n            self.weight.data = pretrained_weights\n        if pretrained_bias is not None:\n            self.bias.data = pretrained_bias\n\n        # Actual trainable parameters\n        if r > 0:\n            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n            self.scaling = self.lora_alpha / self.r\n            self.weight.requires_grad = False\n        self._initialize_lora_parameters()  # Only initialize LoRA parameters\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.transpose(0, 1)\n\n    def _initialize_lora_parameters(self):\n        \"\"\"\n        Initialize only the LoRA-specific parameters (lora_A and lora_B).\n        Avoid reinitializing self.weight or self.bias to preserve pretrained values.\n        \"\"\"\n        if hasattr(self, 'lora_A'):\n            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B)\n            \n    def train(self, mode: bool = True):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n        nn.Linear.train(self, mode)\n        if mode:\n            if self.merge_weights and self.merged:\n                # Make sure that the weights are not merged\n                if self.r > 0:\n                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n                self.merged = False\n        else:\n            if self.merge_weights and not self.merged:\n                # Merge the weights and mark it\n                if self.r > 0:\n                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n                self.merged = True\n\n    def forward(self, x: torch.Tensor):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n        if self.r > 0 and not self.merged:\n            result = F.linear(x, T(self.weight), bias=self.bias)\n            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n            return result\n        else:\n            return F.linear(x, T(self.weight), bias=self.bias)\n\n\n# New Sampling-based Subadapter Linear Layer\nclass SubAdapterLinear(nn.Module):\n    def __init__(self, in_features, out_features, r, s, pretrained_weights=None, pretrained_bias=None):\n        super().__init__()\n        # Base pretrained weights\n        self.weight = nn.Parameter(pretrained_weights.clone())\n        self.bias = nn.Parameter(pretrained_bias.clone()) if pretrained_bias is not None else None\n        # Main adapter matrices\n        self.A = nn.Parameter(torch.zeros(in_features, r))  # in_features x r\n        self.B = nn.Parameter(torch.zeros(r, out_features))  # r x out_features\n        self.r = r\n        self.s = s\n        self._initialize_subadapter()\n\n    def _initialize_subadapter(self):\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.zeros_(self.B)\n\n    def forward(self, x: torch.Tensor):\n        # Base linear output\n        base = F.linear(x, self.weight, bias=self.bias)\n        # Sampling probabilities based on A columns squared norm\n        probs = (self.A ** 2).sum(dim=0)\n        probs = probs / probs.sum()\n        # Sample s indices with replacement\n        idx = torch.multinomial(probs, self.s, replacement=True)\n        # Subadapter pointers (views)\n        S = self.A[:, idx]  # in_features x s\n        R = self.B[idx, :]  # s x out_features\n        sub_out = (x @ S @ R) / self.s\n        return base + sub_out\n\n\ndef replace_linear_with_subadapter(module: nn.Module, parent_name: str = '', skip_substring: str = 'heads.head'):\n    for name, child in list(module.named_children()):\n        module_path = f\"{parent_name}.{name}\" if parent_name else name\n        replace_linear_with_subadapter(child, parent_name=module_path, skip_substring=skip_substring)\n        if isinstance(child, nn.Linear) and skip_substring not in module_path:\n            w = child.weight.data.clone()\n            b = child.bias.data.clone() if child.bias is not None else None\n            sub = SubAdapterLinear(\n                in_features=child.in_features,\n                out_features=child.out_features,\n                r=R_LORA,\n                s=SAMPLING_STEPS,\n                pretrained_weights=w,\n                pretrained_bias=b\n            )\n            setattr(module, name, sub)\n\n\ndef count_trainable_parameters(model):\n    \"\"\"\n    Counts and returns the number of trainable parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef mark_lora_and_head_as_trainable(model: nn.Module, head_substring=\"heads.head\", bias='none'):\n    \"\"\"\n    Unfreeze LoRA parameters + the final classification head (by default `heads.head`).\n    Everything else remains frozen.\n    \"\"\"\n    for name, param in model.named_parameters():\n        # Unfreeze LoRA parameters or SubAdapterLinear parameters A and B\n        if 'lora_' in name or name.endswith('.A') or name.endswith('.B'):\n            param.requires_grad = True\n        # Unfreeze classification head\n        elif head_substring in name:\n            print(\"head_substring came:\", name)\n            param.requires_grad = True\n        # Everything else remains frozen\n        else:\n            param.requires_grad = False\n\n    # Optionally allow some bias fine-tuning\n    if bias == 'all':\n        for n, p in model.named_parameters():\n            if 'bias' in n:\n                p.requires_grad = True\n    elif bias == 'lora_only':\n        for m in model.modules():\n            if isinstance(m, LoRALayer) and hasattr(m, 'bias') and m.bias is not None:\n                m.bias.requires_grad = True\n\n\n# Implement a linear learning rate decay\ndef lr_lambda(current_step: int):\n    \"\"\"\n    Linear decay from step=0 to step=total_steps. At step=0 => 1.0; at step=total_steps => 0.0\n    \"\"\"\n    progress = float(current_step) / float(EPOCHS * len(train_loader))\n    return max(0.0, 1.0 - progress)\n\n\ntorch.manual_seed(17)\n\ntransform = transforms.Compose([ \n    transforms.Resize((224, 224)), \n    transforms.ToTensor()        \n])\ntrain_dir = \"/kaggle/input/tiny-imagenet/tiny-imagenet-200/tiny-imagenet-200/train\"\n\ndataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n\nfrom collections import defaultdict\nimport random\n\n# Few-shot training: select 5 samples per class for the training subset\nlabel_to_indices = defaultdict(list)\nfor idx, (_, label) in enumerate(dataset):\n    label_to_indices[label].append(idx)\nfewshot_indices = []\nfor label, inds in label_to_indices.items():\n    fewshot_indices.extend(random.sample(inds, 5))\n\n# Create train and validation subsets\ntrain_dataset = torch.utils.data.Subset(dataset, fewshot_indices)\nall_indices = set(range(len(dataset)))\nval_indices = list(all_indices - set(fewshot_indices))\nval_dataset = torch.utils.data.Subset(dataset, val_indices)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n# Keep test loader if needed (could point to separate test set)\ntest_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n\n#####################################\n# 2. Model Preparation\n#####################################\n\n\n# Load pre-trained ViT-B/16 weights from torchvision\nmodel = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n\n# Modify the classification head for CIFAR-10 (10 classes)\nnum_features = model.heads.head.in_features\nmodel.heads.head = nn.Sequential(\n    nn.Dropout(DROPOUT),\n    nn.Linear(num_features, 200)   \n)\n\nprint(f\"Number of trainable parameters(total): {count_trainable_parameters(model)}\")\nprint(f\"Number of trainable parameters(heads.head): {count_trainable_parameters(model.heads.head)}\")\nprint(f\"Number of trainable parameters(encoder): {count_trainable_parameters(model.encoder)}\")\nprint(f\"Number of trainable parameters(conv_proj): {count_trainable_parameters(model.conv_proj)}\")\n  \n# Perform the replacement using sampling-based subadapters\nreplace_linear_with_subadapter(model)\nmark_lora_and_head_as_trainable(model, head_substring=\"heads.head\", bias=\"none\")\n\n# Ensure subadapter A and B are trainable as well\nfor module in model.modules():\n    if isinstance(module, SubAdapterLinear):\n        module.A.requires_grad = True\n        module.B.requires_grad = True\n\nprint(f\"Number of trainable parameters(total): {count_trainable_parameters(model)}\")\nprint(f\"Number of trainable parameters(heads.head): {count_trainable_parameters(model.heads.head)}\")\nprint(f\"Number of trainable parameters(encoder): {count_trainable_parameters(model.encoder)}\")\nprint(f\"Number of trainable parameters(conv_proj): {count_trainable_parameters(model.conv_proj)}\")\nprint(model.heads.head)\n\ntrainable_params_list = [name for name, param in model.named_parameters() if param.requires_grad]\nprint(f\"Trainable parameters: {len(trainable_params_list)}\")\nprint(trainable_params_list[:10])  # print first 10 for inspection\n\n#####################################\n# 3. Optimizer & Scheduler\n#####################################\n\n# Filter only trainable (LoRA) parameters\ntrainable_params = filter(lambda p: p.requires_grad, model.parameters())\n\noptimizer = torch.optim.AdamW(trainable_params, lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\n#####################################\n# 4. Training & Validation Loop with Multi-GPU Support\n#####################################\n\n# Check if multiple GPUs are available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)  # Wrap the model for multi-GPU\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n    \nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n\n    for step, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        # combined loss\n        loss_cls = criterion(outputs, labels)\n        reg_loss = 0.0\n        for module in model.modules():\n            if isinstance(module, SubAdapterLinear):\n                A = module.A; B = module.B\n                C = torch.sum(torch.norm(A, dim=0) * torch.norm(B, dim=1))\n                m, _ = A.shape; p = B.shape[1]\n                bound = C / math.sqrt(module.s) * math.sqrt(2 * math.log(m * p) + 2 * math.log(2 / REG_DELTA))\n                # reg_loss = reg_loss + bound\n        loss = loss_cls + REG_LAMBDA * reg_loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        running_loss += loss.item() * images.size(0)\n\n        if step % 10 == 0:\n            current_lr = scheduler.get_last_lr()[0]\n            wandb.log({\n                'Training Loss': loss.item(),\n                'Learning Rate': current_lr\n            }, step=epoch * len(train_loader) + step)\n\n    # After each epoch, perform validation\n    model.eval()\n    all_preds, all_labels = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss_v = criterion(outputs, labels)\n            val_loss += loss_v.item() * images.size(0)\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    val_loss /= len(val_loader.dataset)\n    val_acc = 100.0 * np.mean(np.array(all_preds) == np.array(all_labels))\n    val_f1 = f1_score(all_labels, all_preds, average='macro')\n    wandb.log({\n        'Validation Loss': val_loss,\n        'Validation Accuracy': val_acc,\n        'Validation F1': val_f1    \n    }, step=(epoch+1) * len(train_loader))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:04:49.222372Z","iopub.execute_input":"2025-05-02T18:04:49.222908Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250502_180449-rndtr6r0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/chandan22140-indraprastha-institute-of-information-techn/subadapter_peft/runs/rndtr6r0' target=\"_blank\">fresh-spaceship-4</a></strong> to <a href='https://wandb.ai/chandan22140-indraprastha-institute-of-information-techn/subadapter_peft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/chandan22140-indraprastha-institute-of-information-techn/subadapter_peft' target=\"_blank\">https://wandb.ai/chandan22140-indraprastha-institute-of-information-techn/subadapter_peft</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/chandan22140-indraprastha-institute-of-information-techn/subadapter_peft/runs/rndtr6r0' target=\"_blank\">https://wandb.ai/chandan22140-indraprastha-institute-of-information-techn/subadapter_peft/runs/rndtr6r0</a>"},"metadata":{}},{"name":"stdout","text":"Number of trainable parameters(total): 85952456\nNumber of trainable parameters(heads.head): 153800\nNumber of trainable parameters(encoder): 85207296\nNumber of trainable parameters(conv_proj): 590592\nhead_substring came: heads.head.1.weight\nhead_substring came: heads.head.1.bias\nNumber of trainable parameters(total): 7231688\nNumber of trainable parameters(heads.head): 153800\nNumber of trainable parameters(encoder): 7077888\nNumber of trainable parameters(conv_proj): 0\nSequential(\n  (0): Dropout(p=0.1, inplace=False)\n  (1): Linear(in_features=768, out_features=200, bias=True)\n)\nTrainable parameters: 74\n['encoder.layers.encoder_layer_0.self_attention.out_proj.A', 'encoder.layers.encoder_layer_0.self_attention.out_proj.B', 'encoder.layers.encoder_layer_0.mlp.0.A', 'encoder.layers.encoder_layer_0.mlp.0.B', 'encoder.layers.encoder_layer_0.mlp.3.A', 'encoder.layers.encoder_layer_0.mlp.3.B', 'encoder.layers.encoder_layer_1.self_attention.out_proj.A', 'encoder.layers.encoder_layer_1.self_attention.out_proj.B', 'encoder.layers.encoder_layer_1.mlp.0.A', 'encoder.layers.encoder_layer_1.mlp.0.B']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:04:20.543241Z","iopub.execute_input":"2025-05-02T18:04:20.543940Z","iopub.status.idle":"2025-05-02T18:04:20.547011Z","shell.execute_reply.started":"2025-05-02T18:04:20.543915Z","shell.execute_reply":"2025-05-02T18:04:20.546214Z"}},"outputs":[],"execution_count":16}]}
